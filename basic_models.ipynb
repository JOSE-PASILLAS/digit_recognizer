{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit recognizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/digits.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle provides the train and test sets\n",
    "train_set = pd.read_csv(\"data/train.csv\")\n",
    "test_set = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length and dimensionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape\n",
      "42000  observations by  785  attributes\n",
      "Test set shape\n",
      "28000  observations by  784  attributes\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set shape\")\n",
    "print(train_set.shape[0], \" observations by \", train_set.shape[1], \" attributes\")\n",
    "print(\"Test set shape\")\n",
    "print(test_set.shape[0], \" observations by \", test_set.shape[1], \" attributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training set (training and validation sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store output variable in vector\n",
    "y_train = train_set['label']\n",
    "# Delete attribute label from training set\n",
    "X_train=train_set.drop(['label'], axis=1)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.35, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Labels frequency distribution \n",
    "y_train.hist()\n",
    "plt.xlabel(\"Output label\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title('Labels distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a single case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Establish figure size\n",
    "plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "# Select only first observation and reshape to 28  * 28 matrix\n",
    "sampleimage =  X_train.iloc[1].values.reshape(28, 28)\n",
    "# Plot image in grey scale\n",
    "im = plt.imshow(sampleimage, cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alfa2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the logisti regression model\n",
    "model_log = LogisticRegression(multi_class=\"auto\",random_state=9)\n",
    "# Fit the LR model\n",
    "model_log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set accuracy\n",
    "model_log.score(X_val, y_val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test_set predictions\n",
    "predictions = model_log.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format predictions (Kaggle submission)\n",
    "predictions = pd.DataFrame({'ImageId': list(range(1, len(predictions)+1)),'label': predictions})\n",
    "prediction = pd.DataFrame(predictions).to_csv('D:/kaggle_submissions/predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle public score : 0.89628"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering for dimensionalty reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alfa2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\alfa2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('kmeans',\n",
       "                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\n",
       "                        max_iter=300, n_clusters=90, n_init=10, n_jobs=None,\n",
       "                        precompute_distances='auto', random_state=None,\n",
       "                        tol=0.0001, verbose=0)),\n",
       "                ('log_reg',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess data with K means prior to trainning\n",
    "pipeline = Pipeline([\n",
    "    (\"kmeans\", KMeans(n_clusters=90)),\n",
    "    (\"log_reg\", LogisticRegression()),\n",
    "])\n",
    "# Fit a logistic regression model\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9305442176870748"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation set accuracy\n",
    "pipeline.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test_set predictions\n",
    "k_means_preds = pipeline.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format predictions (Kaggle submission)\n",
    "k_means_preds = pd.DataFrame({'ImageId': list(range(1, len(k_means_preds)+1)),'label': k_means_preds})\n",
    "k_means_pred = pd.DataFrame(k_means_preds).to_csv('D:/kaggle_submissions/k_means_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle public score : 0.92757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Sequential model to build the model layer by layer\n",
    "model = Sequential()\n",
    "# Conv layer with 32 nodes with a filter matrix of 5 x 5\n",
    "model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='Valid', activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='Same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(5, 5), padding='Valid', activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Connection between convolution and dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(519, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "# 0 to 9 possible outcomes\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-3), metrics=[\"accuracy\"])\n",
    "\n",
    "annealer = ReduceLROnPlateau(monitor='val_acc', patience=1, verbose=2, factor=0.5, min_lr=0.0000001) #patience=2\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_train / 255.0\n",
    "\n",
    "#Reshape train, val and test valuest o shape 28 x 28 greyscale\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "X_val = X_val.values.reshape(-1,28,28,1)\n",
    "X_test = test_set.values.reshape(-1,28,28,1)\n",
    "\n",
    "# One-hot encoding output variavle\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_val = to_categorical(y_val, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "682/682 [==============================] - 24s 35ms/step - loss: 0.4119 - accuracy: 0.8658 - val_loss: 0.1341 - val_accuracy: 0.9721\n",
      "Epoch 2/60\n",
      "  4/682 [..............................] - ETA: 28s - loss: 0.1391 - accuracy: 0.9500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alfa2\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\callbacks\\callbacks.py:1042: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682/682 [==============================] - 23s 34ms/step - loss: 0.1183 - accuracy: 0.9650 - val_loss: 0.0515 - val_accuracy: 0.9832\n",
      "Epoch 3/60\n",
      "682/682 [==============================] - 23s 34ms/step - loss: 0.0846 - accuracy: 0.9751 - val_loss: 0.0705 - val_accuracy: 0.9800\n",
      "Epoch 4/60\n",
      "682/682 [==============================] - 23s 34ms/step - loss: 0.0735 - accuracy: 0.9782 - val_loss: 0.0130 - val_accuracy: 0.9844\n",
      "Epoch 5/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0645 - accuracy: 0.9813 - val_loss: 0.0209 - val_accuracy: 0.9851\n",
      "Epoch 6/60\n",
      "682/682 [==============================] - 23s 34ms/step - loss: 0.0542 - accuracy: 0.9832 - val_loss: 0.1326 - val_accuracy: 0.9854\n",
      "Epoch 7/60\n",
      "682/682 [==============================] - 23s 34ms/step - loss: 0.0538 - accuracy: 0.9841 - val_loss: 0.0026 - val_accuracy: 0.9867\n",
      "Epoch 8/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0530 - accuracy: 0.9843 - val_loss: 0.0371 - val_accuracy: 0.9878\n",
      "Epoch 9/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0448 - accuracy: 0.9864 - val_loss: 0.0021 - val_accuracy: 0.9865\n",
      "Epoch 10/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0484 - accuracy: 0.9857 - val_loss: 0.0862 - val_accuracy: 0.9857\n",
      "Epoch 11/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0473 - accuracy: 0.9859 - val_loss: 0.0428 - val_accuracy: 0.9888\n",
      "Epoch 12/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0408 - accuracy: 0.9886 - val_loss: 0.0031 - val_accuracy: 0.9873\n",
      "Epoch 13/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0413 - accuracy: 0.9881 - val_loss: 0.0597 - val_accuracy: 0.9884\n",
      "Epoch 14/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0402 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9889\n",
      "Epoch 15/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0370 - accuracy: 0.9891 - val_loss: 0.0025 - val_accuracy: 0.9910\n",
      "Epoch 16/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0342 - accuracy: 0.9896 - val_loss: 0.0110 - val_accuracy: 0.9903\n",
      "Epoch 17/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0390 - accuracy: 0.9899 - val_loss: 0.0067 - val_accuracy: 0.9860\n",
      "Epoch 18/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0333 - accuracy: 0.9901 - val_loss: 0.0012 - val_accuracy: 0.9896\n",
      "Epoch 19/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0340 - accuracy: 0.9897 - val_loss: 0.0017 - val_accuracy: 0.9905\n",
      "Epoch 20/60\n",
      "682/682 [==============================] - 23s 34ms/step - loss: 0.0364 - accuracy: 0.9892 - val_loss: 0.0073 - val_accuracy: 0.9902\n",
      "Epoch 21/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0298 - accuracy: 0.9904 - val_loss: 2.5053e-04 - val_accuracy: 0.9918\n",
      "Epoch 22/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0344 - accuracy: 0.9891 - val_loss: 4.2987e-04 - val_accuracy: 0.9897\n",
      "Epoch 23/60\n",
      "682/682 [==============================] - 23s 34ms/step - loss: 0.0337 - accuracy: 0.9903 - val_loss: 0.0190 - val_accuracy: 0.9918\n",
      "Epoch 24/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0286 - accuracy: 0.9914 - val_loss: 0.0043 - val_accuracy: 0.9920\n",
      "Epoch 25/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0302 - accuracy: 0.9909 - val_loss: 0.0465 - val_accuracy: 0.9893\n",
      "Epoch 26/60\n",
      "682/682 [==============================] - 23s 34ms/step - loss: 0.0306 - accuracy: 0.9904 - val_loss: 4.8540e-04 - val_accuracy: 0.9920\n",
      "Epoch 27/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0334 - accuracy: 0.9904 - val_loss: 0.0676 - val_accuracy: 0.9912\n",
      "Epoch 28/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0314 - accuracy: 0.9914 - val_loss: 4.8936e-04 - val_accuracy: 0.9930\n",
      "Epoch 29/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0300 - accuracy: 0.9910 - val_loss: 0.0401 - val_accuracy: 0.9906\n",
      "Epoch 30/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0304 - accuracy: 0.9910 - val_loss: 0.0040 - val_accuracy: 0.9916\n",
      "Epoch 31/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0287 - accuracy: 0.9913 - val_loss: 1.9384e-05 - val_accuracy: 0.9917\n",
      "Epoch 32/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0295 - accuracy: 0.9911 - val_loss: 8.4306e-04 - val_accuracy: 0.9929\n",
      "Epoch 33/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0266 - accuracy: 0.9921 - val_loss: 7.1702e-05 - val_accuracy: 0.9911\n",
      "Epoch 34/60\n",
      "682/682 [==============================] - 23s 34ms/step - loss: 0.0309 - accuracy: 0.9912 - val_loss: 0.0723 - val_accuracy: 0.9930\n",
      "Epoch 35/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0290 - accuracy: 0.9916 - val_loss: 0.0193 - val_accuracy: 0.9915\n",
      "Epoch 36/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0308 - accuracy: 0.9912 - val_loss: 0.0550 - val_accuracy: 0.9909\n",
      "Epoch 37/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0261 - accuracy: 0.9920 - val_loss: 0.1484 - val_accuracy: 0.9896\n",
      "Epoch 38/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0266 - accuracy: 0.9920 - val_loss: 0.0016 - val_accuracy: 0.9920\n",
      "Epoch 39/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0253 - accuracy: 0.9922 - val_loss: 0.0292 - val_accuracy: 0.9908\n",
      "Epoch 40/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0318 - accuracy: 0.9911 - val_loss: 0.1067 - val_accuracy: 0.9912\n",
      "Epoch 41/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0245 - accuracy: 0.9925 - val_loss: 0.1797 - val_accuracy: 0.9917\n",
      "Epoch 42/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0279 - accuracy: 0.9916 - val_loss: 7.8525e-04 - val_accuracy: 0.9912\n",
      "Epoch 43/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0272 - accuracy: 0.9917 - val_loss: 1.2195e-05 - val_accuracy: 0.9916\n",
      "Epoch 44/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0284 - accuracy: 0.9911 - val_loss: 0.1057 - val_accuracy: 0.9926\n",
      "Epoch 45/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0240 - accuracy: 0.9932 - val_loss: 0.0017 - val_accuracy: 0.9937\n",
      "Epoch 46/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0271 - accuracy: 0.9921 - val_loss: 0.1227 - val_accuracy: 0.9918\n",
      "Epoch 47/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0241 - accuracy: 0.9928 - val_loss: 4.1488e-04 - val_accuracy: 0.9924\n",
      "Epoch 48/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0249 - accuracy: 0.9928 - val_loss: 0.0030 - val_accuracy: 0.9917\n",
      "Epoch 49/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0255 - accuracy: 0.9928 - val_loss: 7.9713e-04 - val_accuracy: 0.9920\n",
      "Epoch 50/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0312 - accuracy: 0.9915 - val_loss: 9.6432e-06 - val_accuracy: 0.9913\n",
      "Epoch 51/60\n",
      "682/682 [==============================] - 23s 34ms/step - loss: 0.0276 - accuracy: 0.9920 - val_loss: 4.3764e-05 - val_accuracy: 0.9918\n",
      "Epoch 52/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0246 - accuracy: 0.9934 - val_loss: 1.6264e-05 - val_accuracy: 0.9951\n",
      "Epoch 53/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0264 - accuracy: 0.9924 - val_loss: 1.7373e-04 - val_accuracy: 0.9902\n",
      "Epoch 54/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0278 - accuracy: 0.9920 - val_loss: 0.0013 - val_accuracy: 0.9917\n",
      "Epoch 55/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0273 - accuracy: 0.9923 - val_loss: 0.1622 - val_accuracy: 0.9922\n",
      "Epoch 56/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0270 - accuracy: 0.9919 - val_loss: 6.6844e-06 - val_accuracy: 0.9910\n",
      "Epoch 57/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0261 - accuracy: 0.9923 - val_loss: 1.2568e-04 - val_accuracy: 0.9929\n",
      "Epoch 58/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0293 - accuracy: 0.9918 - val_loss: 4.4328e-05 - val_accuracy: 0.9894\n",
      "Epoch 59/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0273 - accuracy: 0.9921 - val_loss: 0.1130 - val_accuracy: 0.9916\n",
      "Epoch 60/60\n",
      "682/682 [==============================] - 23s 33ms/step - loss: 0.0260 - accuracy: 0.9925 - val_loss: 0.0034 - val_accuracy: 0.9931\n",
      "14700/14700 [==============================] - 2s 135us/step\n",
      "Validation accuracy:  0.994557797908783\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "batch_size = 40\n",
    "validation_steps = 10000\n",
    "\n",
    "# initialize Model, Annealer and Datagen\n",
    "#model, annealer, datagen = init_model()\n",
    "\n",
    "# Start training\n",
    "train_generator = datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "test_generator = datagen.flow(X_val, y_val, batch_size=batch_size)\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=X_train.shape[0]//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=validation_steps//batch_size,\n",
    "                    callbacks=[annealer])\n",
    "\n",
    "score = model.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_preds = np.argmax(model.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format predictions (Kaggle submission)\n",
    "cnn_preds = pd.DataFrame({'ImageId': list(range(1, len(cnn_preds)+1)),'label': cnn_preds})\n",
    "cnn_preds = pd.DataFrame(cnn_preds).to_csv('D:/kaggle_submissions/cnn_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle public score : 0.99271"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
